{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35484b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from lxml import html\n",
    "import regex\n",
    "\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca7593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - back out search terms from log file\n",
    "# - resume search\n",
    "# - stopping rule when reached approximate number of jobs\n",
    "# - add argparse for more locations\n",
    "\n",
    "src = '/home/qcx201/Projects/jobs/linkedin/log/get_jobposts-vancouver.log'\n",
    "with open(src, 'r') as f:\n",
    "    log = f.read()\n",
    "\n",
    "lines = log.split('\\n')\n",
    "lines = [line for line in lines if 'search-keyword' in line]\n",
    "keywords = [ln.split(': ')[-1] for ln in lines]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_jobs(params):\n",
    "    \"\"\"\n",
    "    Generator function to scrape job posts from LinkedIn. Limited to 1000 results search parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    # URL for LinkedIn job posts\n",
    "    # Note: This URL may change, and scraping LinkedIn may violate their terms of service.\n",
    "    # Use responsibly and consider using their official API if available.\n",
    "    url = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search'\n",
    "    headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    # request the job postings\n",
    "    sleep = 1\n",
    "    while True:\n",
    "        resp = requests.get(url, params=params, headers=headers)\n",
    "        if resp.status_code == 429:\n",
    "            time.sleep(sleep)\n",
    "            sleep += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # exit function if bad response or empty HTML document\n",
    "    if (not resp) or ('<!DOCTYPE html>\\n\\n<!---->' in resp.text):\n",
    "        print('-'*50)\n",
    "        print(f'request-url [status {resp.status_code}]: {resp.url}')\n",
    "        print('No more jobs found or an error occurred.')\n",
    "        yield\n",
    "\n",
    "    # iterate over job list items\n",
    "    doc = html.fromstring(resp.content)\n",
    "    lis = doc.xpath('//li')\n",
    "    \n",
    "    for li in lis:\n",
    "\n",
    "        # get the text content of the job posting\n",
    "        texts = li.xpath('.//text()')\n",
    "        texts = [regex.sub(r'\\s+', ' ', x).strip() for x in texts]\n",
    "        \n",
    "        # remove empty strings\n",
    "        texts = [x for x in texts if x] \n",
    "\n",
    "        # remove duplicate text\n",
    "        texts = list(dict.fromkeys(texts))\n",
    "\n",
    "        # get links of job postings\n",
    "        anchors = li.xpath('.//a')\n",
    "        hrefs = [a.get('href') for a in anchors]\n",
    "\n",
    "        # yield result\n",
    "        res = {\n",
    "            'position' : texts[0]   if len(texts) > 0 else None,\n",
    "            'company' : texts[1]    if len(texts) > 1 else None,\n",
    "            'location' : texts[2]   if len(texts) > 2 else None,\n",
    "            'status' : texts[3]     if len(texts) > 3 else None,\n",
    "            'job_url' : hrefs[0].split('?')[0],\n",
    "            'firm_url': hrefs[1]    if len(hrefs) > 1 else None,\n",
    "        }\n",
    "\n",
    "        yield res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq(data):\n",
    "    \n",
    "    # df = pandas.DataFrame(data)\n",
    "    df = pandas.DataFrame(data)\n",
    "\n",
    "    # get keywords from job postings\n",
    "    pos_str = ' '.join(df['position'].tolist())\n",
    "    comp_str = ' '.join(df['company'].tolist())\n",
    "    join_str = pos_str + ' ' + comp_str\n",
    "    join_str = join_str.lower()\n",
    "\n",
    "    # remove non-alphabetic characters\n",
    "    remove = [c for c in set(join_str) if not c.isalpha()]\n",
    "    for c in remove:\n",
    "        join_str = join_str.replace(c, ' ')\n",
    "\n",
    "    words = join_str.split()\n",
    "\n",
    "    # get word frequencies\n",
    "    res = [{'keyword' : w, 'count' : words.count(w)} for w in set(words) if len(w) > 2]\n",
    "    return res\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2132ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs_by_location(location):\n",
    "    \"\"\"\n",
    "    Get job postings for a specific location.\n",
    "    \"\"\"\n",
    "\n",
    "    # approx. number of job postings\n",
    "    url = f'https://www.linkedin.com/jobs/search?location={location}'\n",
    "    resp = requests.get(url)\n",
    "    doc = html.fromstring(resp.content)\n",
    "    njobs = doc.xpath('//span[@class=\"results-context-header__job-count\"]//text()')[0]\n",
    "\n",
    "    nskip, ncount = 0, 0\n",
    "\n",
    "    # initialize data structures\n",
    "    data = []\n",
    "    job_urls = set()\n",
    "    searched_kws = set()\n",
    "    kw = ''\n",
    "\n",
    "    # outer loop\n",
    "    while True:\n",
    "\n",
    "        # inner loop\n",
    "        run_search, start = True, 0\n",
    "\n",
    "        while run_search:\n",
    "\n",
    "            params = {\n",
    "                'location': location,\n",
    "                'start': start,\n",
    "                'keywords' : kw,\n",
    "                }\n",
    "            \n",
    "            search_results = search_jobs(params)\n",
    "            \n",
    "            for search_res in search_results:\n",
    "\n",
    "                # break inner loop if no more results\n",
    "                start += 1\n",
    "                ncount += 1\n",
    "                if not search_res:\n",
    "                    run_search = False\n",
    "                    break\n",
    "\n",
    "                # check if the job url is already added\n",
    "                job_url = search_res['job_url'].split('?')[0]\n",
    "                \n",
    "                if job_url not in job_urls:\n",
    "                    job_urls.add(job_url)\n",
    "                    data.append(search_res)\n",
    "                    print(f'[job {len(data):,} of {njobs}]', job_url)\n",
    "\n",
    "                # else:\n",
    "                #     nskip += 1\n",
    "                #     print(f'[skip {nskip:,} of {ncount:,}]', job_url)\n",
    "\n",
    "        # save data\n",
    "        df = pandas.DataFrame(data)\n",
    "        \n",
    "        date = datetime.now().strftime('%Y%m%d')\n",
    "        loc = ''.join(c for c in location if c.isalpha())\n",
    "        \n",
    "        dst = f'../data/jobposts/{date}-{loc}.csv'\n",
    "        df.to_csv(dst, index=False)\n",
    "\n",
    "        print(f'Saved {df.shape}:', dst)\n",
    "        print('Now:', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        print('='*50)\n",
    "\n",
    "        # get word frequencies\n",
    "        freq = get_freq(data)\n",
    "\n",
    "        # filter top unsearched keyword\n",
    "        wf = pandas.DataFrame(freq)\n",
    "        wf = wf.sort_values(by='count', ascending=False) \n",
    "        cond = ~wf['keyword'].isin(searched_kws)\n",
    "        kws = wf[cond]\n",
    "\n",
    "        # break outer loop if no more new keywords found\n",
    "        if kws.empty:\n",
    "            print('No more keywords found.')\n",
    "            break\n",
    "\n",
    "        kw = kws.iloc[0]['keyword']\n",
    "        searched_kws.add(kw)\n",
    "\n",
    "        print('*'*50)\n",
    "        print(f'search-keyword ({len(searched_kws):,}):', kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce837d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations = [\n",
    "#     'vancouver', 'montreal', 'toronto', 'ottawa',\n",
    "#     'stockholm', 'paris', 'berlin', 'london',\n",
    "#     'new york, NY', 'san francisco',\n",
    "# ]\n",
    "\n",
    "location = 'vancouver'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    print('='*50)\n",
    "    print('Location:', location)\n",
    "    print('Start time:', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    get_jobs_by_location(location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

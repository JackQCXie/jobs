{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bdaa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml import html\n",
    "import requests\n",
    "# pd.set_option('display.max_rows', 20)  # Change 100 to your desired number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99128dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/jobposts/20250623-vancouver.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c87e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count\n",
    "print(df.shape)\n",
    "print(df['job_url'].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b433bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CHARS = 2\n",
    "\n",
    "# clean keywords\n",
    "tmp = df['position'].str.lower() + ' ' + df['company'].str.lower()\n",
    "tmp = tmp.apply(lambda text : ''.join(c if c.isalpha() else ' ' for c in text))\n",
    "tmp = tmp.str.split().apply(set)\n",
    "df['keywords'] = tmp.apply(lambda kws: {w for w in kws if len(w) >= MIN_CHARS})\n",
    "\n",
    "# get word frequencies\n",
    "keywords = ' '.join(df['keywords'].str.join(' ')).split()\n",
    "wf_data = [{'keyword' : w, 'count' : keywords.count(w)} for w in set(keywords)]\n",
    "\n",
    "# rank keywords\n",
    "wf = pd.DataFrame(wf_data)\n",
    "wf = wf.sort_values('count', ascending=False)\n",
    "wf['rank'] = range(len(wf))\n",
    "wf = wf.set_index('keyword')\n",
    "\n",
    "rank_dict = wf.to_dict()['rank']\n",
    "df['word_ranks'] = df['keywords'].apply(lambda words : [rank_dict[word] for word in words if word in rank_dict])\n",
    "df['min_rank'] = df['word_ranks'].apply(lambda x : min(x) if x else None)\n",
    "\n",
    "# characteristic rank of job in Vancouver\n",
    "df['mean_rank'] = df['word_ranks'].apply(lambda x : sum(x)/len(x) if x else None).round(2)\n",
    "df = df.sort_values('mean_rank')\n",
    "\n",
    "df['kws_rank'] = df['keywords'].apply(\n",
    "    lambda kws : ', '.join(\n",
    "        f'{kw} ({rank_dict[kw]:,})' for kw in sorted(kws, key=lambda kw:rank_dict[kw])\n",
    "        )\n",
    "    )\n",
    "\n",
    "df['job_count'] = 1\n",
    "pvt = df.pivot_table(\n",
    "    index=['position', 'company', 'location', 'min_rank', 'mean_rank', 'kws_rank'],\n",
    "    values='job_count',\n",
    "    aggfunc='count'\n",
    "    )\n",
    "\n",
    "pvt = pvt.reset_index()\n",
    "pvt = pvt.sort_values('mean_rank')\n",
    "pvt = pvt.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a93b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "includes = ['policy', 'research', 'data'] # or\n",
    "excludes = ['assistant', 'university', 'senior', 'doctoral', 'intern', 'phd'] # and\n",
    "\n",
    "# initialize condition\n",
    "cond = pvt['kws_rank'].isna()\n",
    "\n",
    "for w in includes:\n",
    "    cond = cond | (pvt['kws_rank'].str.contains(w))\n",
    "\n",
    "for w in excludes:\n",
    "    cond = cond & (~pvt['kws_rank'].str.contains(w))\n",
    "\n",
    "# candidate positions\n",
    "res = pvt[cond].reset_index(drop=True)\n",
    "\n",
    "on = ['position', 'company', 'location']\n",
    "res = pd.merge(res, df[on+['job_url', 'firm_url']], on=on, how='inner', )\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba8de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in res.iloc:\n",
    "    print(entry['job_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b1ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def extract_job_criteria(tree):\n",
    "    \"\"\"Extract job criteria items using more robust XPath\"\"\"\n",
    "    criteria = {}\n",
    "    items = tree.xpath('//li[contains(@class, \"description__job-criteria-item\")]')\n",
    "    \n",
    "    for item in items:\n",
    "        header = item.xpath('.//h3[contains(@class, \"description__job-criteria-subheader\")]/text()')\n",
    "        value = item.xpath('.//span[contains(@class, \"description__job-criteria-text\")]/text()')\n",
    "        \n",
    "        if header and value:\n",
    "            key = header[0].strip().lower().replace(' ', '_')\n",
    "            criteria[key] = value[0].strip()\n",
    "    \n",
    "    return criteria\n",
    "\n",
    "def extract_salary(tree, description):\n",
    "    \"\"\"Extract salary information from both dedicated element and description\"\"\"\n",
    "    # Try dedicated salary element\n",
    "    salary_element = tree.xpath('//div[contains(@class, \"salary\")]/text()')\n",
    "    if salary_element:\n",
    "        return salary_element[0].strip()\n",
    "    \n",
    "    # Try regex patterns in description\n",
    "    patterns = [\n",
    "        r'\\$\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?',  # $XX,XXX.XX\n",
    "        r'\\$\\d{1,3}(?:,\\d{3})*\\s*[-â€“]\\s*\\$\\d{1,3}(?:,\\d{3})*',  # $XX - $YY\n",
    "        r'\\d{1,3}(?:,\\d{3})*\\s*(?:USD|CAD)',  # XX,XXX CAD\n",
    "        r'[A-Z]{3}\\s?\\d{1,3}(?:,\\d{3})*'  # CAD XX,XXX\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, description, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def scrape_job(url, max_retries=3, base_delay=1):\n",
    "    \"\"\"Scrape job information with simplified rate limiting\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    }\n",
    "    \n",
    "    job_data = {\n",
    "        \"url\": url,\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"job_title\": None,\n",
    "        \"company\": None,\n",
    "        \"location\": None,\n",
    "        \"posted_time\": None,\n",
    "        \"job_description\": None,\n",
    "        \"applicant_count\": None,\n",
    "        \"employer_profile\": None,\n",
    "        \"apply_link\": None,\n",
    "        \"salary\": None,\n",
    "        \"seniority_level\": None,\n",
    "        \"employment_type\": None,\n",
    "        \"job_function\": None,\n",
    "        \"industries\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = None\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, timeout=15)\n",
    "                if response.status_code == 429:\n",
    "                    time.sleep(attempt + base_delay)  # Linear backoff\n",
    "                    continue\n",
    "                response.raise_for_status()\n",
    "                break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep(attempt + base_delay)\n",
    "        \n",
    "        tree = html.fromstring(response.content)\n",
    "        \n",
    "        # Extract core job information\n",
    "        job_data[\"job_title\"] = safe_extract(tree, '//h1[contains(@class, \"top-card-layout__title\")]/text()')\n",
    "        job_data[\"company\"] = safe_extract(tree, '//a[contains(@class, \"topcard__org-name-link\")]/text()')\n",
    "        job_data[\"location\"] = safe_extract(tree, '//span[contains(@class, \"topcard__flavor--bullet\")]/text()')\n",
    "        job_data[\"posted_time\"] = safe_extract(tree, '//span[contains(@class, \"posted-time-ago__text\")]/text()')\n",
    "        \n",
    "        # Extract job description\n",
    "        desc_nodes = tree.xpath('//div[contains(@class, \"show-more-less-html__markup\")]')\n",
    "        job_data[\"job_description\"] = \" \".join(desc_nodes[0].xpath(\".//text()\")).strip() if desc_nodes else None\n",
    "        \n",
    "        # Extract salary information\n",
    "        if job_data[\"job_description\"]:\n",
    "            job_data[\"salary\"] = extract_salary(tree, job_data[\"job_description\"])\n",
    "        \n",
    "        # Extract additional metadata\n",
    "        job_data[\"applicant_count\"] = safe_extract(tree, '//span[contains(@class, \"num-applicants__caption\")]/text()')\n",
    "        job_data[\"employer_profile\"] = safe_extract(tree, '//a[contains(@class, \"topcard__org-name-link\")]/@href')\n",
    "        job_data[\"apply_link\"] = safe_extract(tree, '//a[contains(@class, \"apply-button\")]/@href')\n",
    "        \n",
    "        # Extract job criteria\n",
    "        criteria = extract_job_criteria(tree)\n",
    "        job_data.update({\n",
    "            \"seniority_level\": criteria.get(\"seniority_level\"),\n",
    "            \"employment_type\": criteria.get(\"employment_type\"),\n",
    "            \"job_function\": criteria.get(\"job_function\"),\n",
    "            \"industries\": criteria.get(\"industries\")\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {str(e)}\")\n",
    "    \n",
    "    return job_data\n",
    "\n",
    "def safe_extract(tree, xpath):\n",
    "    \"\"\"Safely extract first match from XPath or return None\"\"\"\n",
    "    result = tree.xpath(xpath)\n",
    "    return result[0].strip() if result else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "JOB_URLS = res['job_url'].tolist()\n",
    "MAX_RETRIES = 10\n",
    "BASE_DELAY = 1  # seconds\n",
    "\n",
    "\n",
    "# Scrape all jobs\n",
    "results = []\n",
    "for i, url in enumerate(JOB_URLS):\n",
    "    \n",
    "    print(f\"Processing {i+1}/{len(JOB_URLS)}: {url}\")\n",
    "    job_data = scrape_job(url, MAX_RETRIES, BASE_DELAY)\n",
    "    results.append(job_data)\n",
    "    \n",
    "    print(json.dumps(job_data, indent=2))\n",
    "\n",
    "# Save results\n",
    "with open(\"linkedin_jobs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Successfully scraped {len(results)} jobs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c64fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
